{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements:\n",
    "- Model (Estimator or Pipeline)\n",
    "- Set of ParamMaps (to perform the grid search) - you should use the ***ParamGridBuilder*** utility\n",
    "- Evaluator (to assess the fitness of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splits the dataset into K folds\n",
    "- Each fold is splitted into a training (2/3) and a test (1/3) sets\n",
    "- It will fit K models and compute the average of the K evaluation metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a VERY computationally expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "training = sqlc.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 10},\n",
       " {Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 100},\n",
       " {Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 1000},\n",
       " {Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 10},\n",
       " {Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 100},\n",
       " {Param(parent='LogisticRegression_414ba203ce9e4989fac8', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='HashingTF_4aaaa107b6b05116b537', name='numFeatures', doc='number of features.'): 1000}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.962962962962963,\n",
       " 0.962962962962963,\n",
       " 0.9259259259259258,\n",
       " 0.962962962962963,\n",
       " 0.962962962962963,\n",
       " 0.9259259259259258]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.bestModel.stages[1].save('awesome1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\":\"org.apache.spark.ml.classification.LogisticRegressionModel\",\"timestamp\":1518882244188,\"sparkVersion\":\"2.2.0\",\"uid\":\"LogisticRegression_414ba203ce9e4989fac8\",\"paramMap\":{\"threshold\":0.5,\"regParam\":0.1,\"aggregationDepth\":2,\"rawPredictionCol\":\"rawPrediction\",\"standardization\":true,\"tol\":1.0E-6,\"fitIntercept\":true,\"predictionCol\":\"prediction\",\"family\":\"auto\",\"featuresCol\":\"features\",\"elasticNetParam\":0.0,\"maxIter\":10,\"probabilityCol\":\"probability\",\"labelCol\":\"label\"}}\r\n"
     ]
    }
   ],
   "source": [
    "!cat awesome2.parquet/metadata/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_4f44b9f0f030d0197044,\n",
       " HashingTF_44f4ba8750770b40f4ea,\n",
       " LogisticRegression_4fe98415bc06b1f16ab7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_best = cvModel.bestModel.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.0059, 0.0124, 0.5976, -0.2633, -0.6609, 1.787, -1.4679, 0.0121, 1.1602, 0.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_best.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_summary = lr_best.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sqlc.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.627, 0.373]), prediction=0.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.3451, 0.6549]), prediction=1.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3351, 0.6649]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.2767, 0.7233]), prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(test)\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It uses the entire dataset\n",
    "- The dataset is splitted into a training and a test sets according to the ***trainRatio*** parameter\n",
    "- It will fit a model for each set of parameters and evaluate its metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a much less expensive, but it may not yield good results if the dataset is not large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "data = sqlc.read.format(\"libsvm\").load(\"/home/ubuntu/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LinearRegression(maxIter=20, regParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0},\n",
       " {Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5},\n",
       " {Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0},\n",
       " {Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0},\n",
       " {Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5},\n",
       " {Param(parent='LinearRegression_4f8e9390c770160f7729', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearRegression_4f8e9390c770160f7729', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_4f8e9390c770160f7729"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.7786, 0.4071, -0.9313, 3.2022, 0.7962, 1.0608, 0.1326, -0.9576, -0.7159, 0.5896])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04953668424664892"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-23.487440</td>\n",
       "      <td>(-0.519535443126, 0.808035794841, 0.8498613208...</td>\n",
       "      <td>-0.886864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.275214</td>\n",
       "      <td>(-0.489685764918, 0.683231434274, 0.9115808714...</td>\n",
       "      <td>0.865535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.065400</td>\n",
       "      <td>(-0.0177244659457, 0.563282914714, 0.142324203...</td>\n",
       "      <td>0.981015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.862009</td>\n",
       "      <td>(-0.634145383179, -0.925918063973, 0.302702923...</td>\n",
       "      <td>1.795393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-15.780685</td>\n",
       "      <td>(-0.256849206372, 0.774097619743, -0.782915810...</td>\n",
       "      <td>3.708987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           features  prediction\n",
       "0 -23.487440  (-0.519535443126, 0.808035794841, 0.8498613208...   -0.886864\n",
       "1 -18.275214  (-0.489685764918, 0.683231434274, 0.9115808714...    0.865535\n",
       "2 -17.065400  (-0.0177244659457, 0.563282914714, 0.142324203...    0.981015\n",
       "3 -15.862009  (-0.634145383179, -0.925918063973, 0.302702923...    1.795393\n",
       "4 -15.780685  (-0.256849206372, 0.774097619743, -0.782915810...    3.708987"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
