{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an introduction in how to use the Tensorflow framework.\n",
    "\n",
    "We will cover the components needed for a simple feedfoward neural network.\n",
    "\n",
    "Key concepts & Tensorflow paradigms covered:\n",
    "- Placeholders\n",
    "- Variable\n",
    "- Initializers\n",
    "- Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we use a set of tuples to define the network structure\n",
    "input_shape =(4,)  #  the length of the input numpy array\n",
    "input_nodes= (6,)  #  the number of nodes in the input layer\n",
    "hidden_nodes = (8,)  \n",
    "output_shape = (4,)  #  num. nodes in output layer and the length of the output numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Tensorflow paradigm is the **Placeholder**.\n",
    "\n",
    "Tensorflow uses placeholders to feed data into the network.\n",
    "\n",
    "The placeholder is fed using numpy arrays.\n",
    "\n",
    "The first dimension is the number of samples in the batch - we use None to be able to input any batch size we want.\n",
    "\n",
    "The second dimension is the shape of one sample - i.e. the length of the input numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = tf.placeholder(tf.float32, shape=(None, *input_shape), name='network_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two Tensorflow paradigms are the **Variable** and **Initializer**.  \n",
    "\n",
    "Variable objects are used to hold tensors of variable that tensorflow can change.  Both weights and biases are tf.Variables.\n",
    "\n",
    "We also need to tell Tensorflow what initial values we want our varibles to be.  \n",
    "\n",
    "To do the initialization we pass in the tf.random_normal initializer.  The shape of the Variable tensor is specified in the intializer.\n",
    "\n",
    "The * operation is used to unpack the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_weights = tf.Variable(tf.random_normal(shape=(*input_shape, *input_nodes), name='input_weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same pattern for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bias = tf.Variable(tf.zeros(shape=(*input_nodes,)), name='input_bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can form the input layer using matrix multiplication between the input & weights, then add the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_activation = tf.add(tf.matmul(network_input, input_weights), input_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can form the layer by squeezing the output through a rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.nn.relu(pre_activation, name='input_layer_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a hidden layer using the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_w = tf.Variable(tf.random_normal((*input_nodes, *hidden_nodes)), name='hidden_weights')\n",
    "h_b = tf.Variable(tf.zeros((*hidden_nodes, ), name='hidden_bias'))\n",
    "hidden_layer = tf.nn.relu(tf.add(tf.matmul(input_layer, h_w), h_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer has no activation function (aka a linear activation function).  This allows the network to output negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_w = tf.Variable(tf.random_normal((*hidden_nodes, *output_shape)), name='output_weights')\n",
    "o_b = tf.Variable(tf.zeros((*output_shape, ), name='output_bias'))\n",
    "output = tf.add(tf.matmul(hidden_layer, o_w), o_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the code above allows us to make predictions with our network - i.e. we can do forward passes across the network.\n",
    "\n",
    "Below we will setup the code for training.\n",
    "\n",
    "We first need another placeholder.  This will serve as the target value for the network (aka y_train) - what our network should be outputting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.placeholder(tf.float32, shape=(None, *output_shape), name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(target, output, scope='loss_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer to do the heavy lifting of training the network.  \n",
    "\n",
    "Here we use the Adam optimizer.  Note that the input here of learning rate is one of the most important in training any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a Tensorflow operation to actually do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the machinery to do forward passes and to back propagate error is in place.  \n",
    "\n",
    "Add in a few Tensorflow summary operations to track what is going on in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.tensor_summary('input', network_input)\n",
    "tf.summary.histogram('input_weights', input_weights)\n",
    "tf.summary.histogram('output_bias', o_b)\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Tensorflow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing our model needs is data.  The function below generates training data for a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generates training data for our network.\n",
    "\n",
    "    args\n",
    "        num_samples (int)\n",
    "\n",
    "    returns\n",
    "        net_in (np.array)  fed into the network (aka features or x)\n",
    "        net_out (np.array)  aka target or y - the value we are trying to approx.\n",
    "    \"\"\"\n",
    "    net_in = np.random.rand(num_samples, *input_shape)\n",
    "    net_out = net_in + 10\n",
    "\n",
    "    return net_in, net_out\n",
    "\n",
    "#  run the function to get data to train with\n",
    "inputs, targets = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce another key Tensorflow paradigm - the Session.\n",
    "\n",
    "You can think of a Session as one instance of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 98.8860855102539\n",
      "step 1 loss 98.59093475341797\n",
      "step 2 loss 98.29656982421875\n",
      "step 3 loss 98.001708984375\n",
      "step 4 loss 97.70490264892578\n",
      "step 5 loss 97.40753173828125\n",
      "step 6 loss 97.11018371582031\n",
      "step 7 loss 96.81302642822266\n",
      "step 8 loss 96.51760864257812\n",
      "step 9 loss 96.22293090820312\n",
      "step 10 loss 95.9277114868164\n",
      "step 11 loss 95.63294219970703\n",
      "step 12 loss 95.3370132446289\n",
      "step 13 loss 95.0410385131836\n",
      "step 14 loss 94.7453842163086\n",
      "step 15 loss 94.44995880126953\n",
      "step 16 loss 94.15433502197266\n",
      "step 17 loss 93.85828399658203\n",
      "step 18 loss 93.56127166748047\n",
      "step 19 loss 93.26449584960938\n",
      "step 20 loss 92.96757507324219\n",
      "step 21 loss 92.67060852050781\n",
      "step 22 loss 92.37322235107422\n",
      "step 23 loss 92.07579040527344\n",
      "step 24 loss 91.77836608886719\n",
      "step 25 loss 91.4795150756836\n",
      "step 26 loss 91.17972564697266\n",
      "step 27 loss 90.87873840332031\n",
      "step 28 loss 90.57742309570312\n",
      "step 29 loss 90.27643585205078\n",
      "step 30 loss 89.97523498535156\n",
      "step 31 loss 89.67547607421875\n",
      "step 32 loss 89.37603759765625\n",
      "step 33 loss 89.07658386230469\n",
      "step 34 loss 88.7773666381836\n",
      "step 35 loss 88.47820281982422\n",
      "step 36 loss 88.17788696289062\n",
      "step 37 loss 87.87716674804688\n",
      "step 38 loss 87.57624816894531\n",
      "step 39 loss 87.27501678466797\n",
      "step 40 loss 86.97171783447266\n",
      "step 41 loss 86.6675796508789\n",
      "step 42 loss 86.36187744140625\n",
      "step 43 loss 86.05510711669922\n",
      "step 44 loss 85.74713897705078\n",
      "step 45 loss 85.43846893310547\n",
      "step 46 loss 85.12891387939453\n",
      "step 47 loss 84.81853485107422\n",
      "step 48 loss 84.50707244873047\n",
      "step 49 loss 84.19493865966797\n",
      "step 50 loss 83.88256072998047\n",
      "step 51 loss 83.56996154785156\n",
      "step 52 loss 83.25687408447266\n",
      "step 53 loss 82.94322967529297\n",
      "step 54 loss 82.62850952148438\n",
      "step 55 loss 82.31224822998047\n",
      "step 56 loss 81.99324035644531\n",
      "step 57 loss 81.67218017578125\n",
      "step 58 loss 81.34984588623047\n",
      "step 59 loss 81.02619171142578\n",
      "step 60 loss 80.70123291015625\n",
      "step 61 loss 80.37493896484375\n",
      "step 62 loss 80.04669952392578\n",
      "step 63 loss 79.71805572509766\n",
      "step 64 loss 79.38927459716797\n",
      "step 65 loss 79.05953216552734\n",
      "step 66 loss 78.72758483886719\n",
      "step 67 loss 78.39404296875\n",
      "step 68 loss 78.05903625488281\n",
      "step 69 loss 77.72315216064453\n",
      "step 70 loss 77.38642120361328\n",
      "step 71 loss 77.0496597290039\n",
      "step 72 loss 76.7109146118164\n",
      "step 73 loss 76.3706283569336\n",
      "step 74 loss 76.02981567382812\n",
      "step 75 loss 75.6883544921875\n",
      "step 76 loss 75.34671020507812\n",
      "step 77 loss 75.00471496582031\n",
      "step 78 loss 74.66202545166016\n",
      "step 79 loss 74.31949615478516\n",
      "step 80 loss 73.97659301757812\n",
      "step 81 loss 73.63339233398438\n",
      "step 82 loss 73.28958892822266\n",
      "step 83 loss 72.9455795288086\n",
      "step 84 loss 72.60106658935547\n",
      "step 85 loss 72.25611114501953\n",
      "step 86 loss 71.91056060791016\n",
      "step 87 loss 71.56478881835938\n",
      "step 88 loss 71.2188949584961\n",
      "step 89 loss 70.87277221679688\n",
      "step 90 loss 70.52497100830078\n",
      "step 91 loss 70.17633819580078\n",
      "step 92 loss 69.8271484375\n",
      "step 93 loss 69.47737121582031\n",
      "step 94 loss 69.12723541259766\n",
      "step 95 loss 68.77674102783203\n",
      "step 96 loss 68.42549896240234\n",
      "step 97 loss 68.07394409179688\n",
      "step 98 loss 67.72154235839844\n",
      "step 99 loss 67.36812591552734\n",
      "[[ 1.4713974   2.8402061   1.9400176   2.5004997 ]\n",
      " [ 0.8745699   6.5503078   1.9717191   7.132088  ]\n",
      " [-1.4175364   6.4465957  -0.10065193  9.81047   ]\n",
      " [ 0.60325277  1.177618    0.9690831   1.947705  ]\n",
      " [-0.6070615   5.292796    0.56611735  6.0373735 ]]\n",
      "[[10.57919557 10.39104043 10.30311173 10.59502296]\n",
      " [10.67894775 10.79829449 10.99305499 10.85121916]\n",
      " [10.34549242 10.00305618 10.99395798 10.35645793]\n",
      " [10.43992415 10.30064398 10.08050492 10.08037423]\n",
      " [10.18254576 10.77428338 10.81517754 10.16146277]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    #  first a necessary bit of Tensorflow boiler plate - initializing variables\n",
    "    #  we do this operation using the Session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #  we create a FileWriter object to write out summarys to a file for Tensorboard to read\n",
    "    writer = tf.summary.FileWriter('./logs', graph=sess.graph)\n",
    "    \n",
    "    #  now the training loop\n",
    "    #  we are not splitting our training data into batches\n",
    "    \n",
    "    for train_step in range(100):\n",
    "        #  now we run the tensorflow graph\n",
    "        #  the graph is run by calling the .run method on the session\n",
    "        #  the run method takes two inputs:\n",
    "        #   fetches = the tf operations to run\n",
    "        #   feed_dict = values for the placeholders\n",
    "\n",
    "        #  here we fetch two operations\n",
    "        #   train_op - the operation to train the network\n",
    "        #   summary - the summary operations for the graph\n",
    "        fetches = [loss, train_op, merged]\n",
    "\n",
    "        #  the feed_dict is a dictionary with\n",
    "        #   keys = the placeholders\n",
    "        #   values = the numpy arrays\n",
    "        #   note that we feed in multiple samples\n",
    "        feed_dict = {network_input: inputs,\n",
    "                     target: targets}\n",
    "        #  finally we run the session using the fetches and feed_dict\n",
    "        loss_value, _, summary = sess.run(fetches, feed_dict)\n",
    "        #  the operation to add the summary to the tensorboard output file\n",
    "        writer.add_summary(summary, train_step)\n",
    "        print('step {} loss {}'.format(train_step, loss_value))        \n",
    "        \n",
    "    #  now training is done\n",
    "    #  generate a test set \n",
    "    test_in, test_out = generate_data(5)\n",
    "\n",
    "    #  here we get predictions from our network - we don't train\n",
    "    pred = sess.run(output, {network_input: test_in})\n",
    "    print(pred)\n",
    "    print(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
