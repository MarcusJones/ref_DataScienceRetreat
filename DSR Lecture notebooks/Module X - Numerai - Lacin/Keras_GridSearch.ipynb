{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import SGD, Adamax\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "#def create_model(learn_rate=0.01, momentum=0):# include this parameter for optimizer tuning\n",
    "#def create_model(activation='relu'):\n",
    "#def create_model(init_mode='uniform'):\n",
    "#def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "def create_model(neurons=500, neurons2=100,  neurons3=50):   \n",
    "    \n",
    "    optimizer = Adamax(lr=0.001)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=103,kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(neurons2, kernel_initializer='uniform',activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(neurons3, kernel_initializer='uniform',activation='relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(9, kernel_initializer='uniform',activation='softmax'))\n",
    "    # Compile model\n",
    "    #optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # set parameter for opt tuning\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Epochs Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "datadir= './task/'\n",
    "df_train = pd.read_csv( datadir + 'trainData.csv')   \n",
    "df_test = pd.read_csv( datadir + 'testData.csv')\n",
    "\n",
    "feature_cols = df_train.columns[0:-1]\n",
    "target_col = df_train.columns[-1]\n",
    "X=df_train[feature_cols].values\n",
    "y=df_train[target_col].values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "# convert integers to dummy variables \n",
    "y = np_utils.to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.801109 using {'batch_size': 64, 'epochs': 20}\n",
      "0.800430 (0.004486) with: {'batch_size': 16, 'epochs': 10}\n",
      "0.796325 (0.006982) with: {'batch_size': 16, 'epochs': 20}\n",
      "0.793384 (0.002247) with: {'batch_size': 16, 'epochs': 30}\n",
      "0.799363 (0.001789) with: {'batch_size': 32, 'epochs': 10}\n",
      "0.798442 (0.003209) with: {'batch_size': 32, 'epochs': 20}\n",
      "0.794887 (0.002170) with: {'batch_size': 32, 'epochs': 30}\n",
      "0.799008 (0.002625) with: {'batch_size': 64, 'epochs': 10}\n",
      "0.801109 (0.001831) with: {'batch_size': 64, 'epochs': 20}\n",
      "0.794046 (0.002026) with: {'batch_size': 64, 'epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [16, 32, 64] # if not in the interval continue to grid seacrh\n",
    "epochs = [10, 20, 30]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Gridsearch\n",
    "\n",
    "build fn create model is called via an optimizer parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.524376 using {'optimizer': 'Adamax'}\n",
      "-0.638321 (0.005728) with: {'optimizer': 'SGD'}\n",
      "-0.541467 (0.019526) with: {'optimizer': 'RMSprop'}\n",
      "-0.545167 (0.002207) with: {'optimizer': 'Adagrad'}\n",
      "-0.537486 (0.001481) with: {'optimizer': 'Adadelta'}\n",
      "-0.531548 (0.000412) with: {'optimizer': 'Adam'}\n",
      "-0.524376 (0.000285) with: {'optimizer': 'Adamax'}\n",
      "-0.577695 (0.008125) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.531190 using {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "-0.945211 (0.012189) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "-0.867184 (0.008023) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "-0.806572 (0.004293) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "-0.748517 (0.005261) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "-0.688189 (0.003425) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "-0.637929 (0.004332) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "-0.637241 (0.003389) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "-0.621973 (0.002175) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "-0.609467 (0.003353) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "-0.577011 (0.001034) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "-0.560955 (0.004908) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "-0.531190 (0.000946) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "-0.539675 (0.003205) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "-0.541772 (0.008990) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "-0.553682 (0.015555) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "-0.549232 (0.012887) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "-0.554072 (0.009528) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "-0.585635 (0.010724) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "-0.550041 (0.015875) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "-0.549499 (0.007661) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "-0.549013 (0.009308) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "-0.566437 (0.020290) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "-0.578462 (0.010205) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "-0.623137 (0.011986) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "-0.550525 (0.010384) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "-0.545682 (0.004921) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "-0.570464 (0.018914) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "-0.579267 (0.013708) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "-0.608161 (0.006593) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "-0.687967 (0.022659) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9] #0.9 a popular value in practice\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.508739 using {'epochs': 17, 'neurons': 600, 'neurons2': 200}\n",
      "-0.522193 (0.006182) with: {'epochs': 8, 'neurons': 600, 'neurons2': 300}\n",
      "-0.523965 (0.001720) with: {'epochs': 8, 'neurons': 600, 'neurons2': 200}\n",
      "-0.513871 (0.001651) with: {'epochs': 8, 'neurons': 800, 'neurons2': 300}\n",
      "-0.517763 (0.001434) with: {'epochs': 8, 'neurons': 800, 'neurons2': 200}\n",
      "-0.516376 (0.007269) with: {'epochs': 17, 'neurons': 600, 'neurons2': 300}\n",
      "-0.508739 (0.003769) with: {'epochs': 17, 'neurons': 600, 'neurons2': 200}\n",
      "-0.525197 (0.001602) with: {'epochs': 17, 'neurons': 800, 'neurons2': 300}\n",
      "-0.510729 (0.001342) with: {'epochs': 17, 'neurons': 800, 'neurons2': 200}\n",
      "-0.550526 (0.008768) with: {'epochs': 23, 'neurons': 600, 'neurons2': 300}\n",
      "-0.539632 (0.011313) with: {'epochs': 23, 'neurons': 600, 'neurons2': 200}\n",
      "-0.566808 (0.011942) with: {'epochs': 23, 'neurons': 800, 'neurons2': 300}\n",
      "-0.545097 (0.005798) with: {'epochs': 23, 'neurons': 800, 'neurons2': 200}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model,epochs=20, batch_size=100, verbose=0)\n",
    "neurons = [600,800]\n",
    "neurons2=[300,200]\n",
    "epochs = [8, 17, 23]\n",
    "param_grid = dict(neurons=neurons, neurons2=neurons2, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.505109 using {'epochs': 17, 'neurons': 650, 'neurons2': 75}\n",
      "-0.510174 (0.003086) with: {'epochs': 14, 'neurons': 600, 'neurons2': 200}\n",
      "-0.509185 (0.004402) with: {'epochs': 14, 'neurons': 600, 'neurons2': 100}\n",
      "-0.509345 (0.001641) with: {'epochs': 14, 'neurons': 600, 'neurons2': 75}\n",
      "-0.507119 (0.003324) with: {'epochs': 14, 'neurons': 650, 'neurons2': 200}\n",
      "-0.510551 (0.002601) with: {'epochs': 14, 'neurons': 650, 'neurons2': 100}\n",
      "-0.508348 (0.001361) with: {'epochs': 14, 'neurons': 650, 'neurons2': 75}\n",
      "-0.522969 (0.007784) with: {'epochs': 17, 'neurons': 600, 'neurons2': 200}\n",
      "-0.508614 (0.001468) with: {'epochs': 17, 'neurons': 600, 'neurons2': 100}\n",
      "-0.508832 (0.003991) with: {'epochs': 17, 'neurons': 600, 'neurons2': 75}\n",
      "-0.512417 (0.009104) with: {'epochs': 17, 'neurons': 650, 'neurons2': 200}\n",
      "-0.506792 (0.004205) with: {'epochs': 17, 'neurons': 650, 'neurons2': 100}\n",
      "-0.505109 (0.003746) with: {'epochs': 17, 'neurons': 650, 'neurons2': 75}\n",
      "-0.526323 (0.013259) with: {'epochs': 20, 'neurons': 600, 'neurons2': 200}\n",
      "-0.510335 (0.005396) with: {'epochs': 20, 'neurons': 600, 'neurons2': 100}\n",
      "-0.506507 (0.002764) with: {'epochs': 20, 'neurons': 600, 'neurons2': 75}\n",
      "-0.527145 (0.008248) with: {'epochs': 20, 'neurons': 650, 'neurons2': 200}\n",
      "-0.515410 (0.013141) with: {'epochs': 20, 'neurons': 650, 'neurons2': 100}\n",
      "-0.512901 (0.001414) with: {'epochs': 20, 'neurons': 650, 'neurons2': 75}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model,epochs=20, batch_size=96, verbose=0)\n",
    "neurons = [600,650]\n",
    "neurons2=[200,100,75]\n",
    "epochs = [14, 17, 20]\n",
    "param_grid = dict(neurons=neurons, neurons2=neurons2, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Neuron Activation Function (relu generally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "#param_grid = dict(activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.503397 using {'batch_size': 256, 'epochs': 19, 'neurons': 1000, 'neurons2': 100}\n",
      "-0.504018 (0.002077) with: {'batch_size': 128, 'epochs': 19, 'neurons': 900, 'neurons2': 80}\n",
      "-0.511930 (0.006733) with: {'batch_size': 128, 'epochs': 19, 'neurons': 900, 'neurons2': 100}\n",
      "-0.504600 (0.001919) with: {'batch_size': 128, 'epochs': 19, 'neurons': 1000, 'neurons2': 80}\n",
      "-0.506462 (0.003359) with: {'batch_size': 128, 'epochs': 19, 'neurons': 1000, 'neurons2': 100}\n",
      "-0.511937 (0.001237) with: {'batch_size': 128, 'epochs': 22, 'neurons': 900, 'neurons2': 80}\n",
      "-0.514222 (0.006187) with: {'batch_size': 128, 'epochs': 22, 'neurons': 900, 'neurons2': 100}\n",
      "-0.511364 (0.004483) with: {'batch_size': 128, 'epochs': 22, 'neurons': 1000, 'neurons2': 80}\n",
      "-0.520178 (0.007607) with: {'batch_size': 128, 'epochs': 22, 'neurons': 1000, 'neurons2': 100}\n",
      "-0.505698 (0.001082) with: {'batch_size': 256, 'epochs': 19, 'neurons': 900, 'neurons2': 80}\n",
      "-0.512080 (0.003767) with: {'batch_size': 256, 'epochs': 19, 'neurons': 900, 'neurons2': 100}\n",
      "-0.509300 (0.001418) with: {'batch_size': 256, 'epochs': 19, 'neurons': 1000, 'neurons2': 80}\n",
      "-0.503397 (0.002112) with: {'batch_size': 256, 'epochs': 19, 'neurons': 1000, 'neurons2': 100}\n",
      "-0.507196 (0.002807) with: {'batch_size': 256, 'epochs': 22, 'neurons': 900, 'neurons2': 80}\n",
      "-0.505598 (0.002067) with: {'batch_size': 256, 'epochs': 22, 'neurons': 900, 'neurons2': 100}\n",
      "-0.509717 (0.004638) with: {'batch_size': 256, 'epochs': 22, 'neurons': 1000, 'neurons2': 80}\n",
      "-0.506988 (0.004768) with: {'batch_size': 256, 'epochs': 22, 'neurons': 1000, 'neurons2': 100}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model,epochs=17, batch_size=96, verbose=0)\n",
    "neurons = [1000,1200]\n",
    "neurons2=[100,200]\n",
    "neurons3=[50,75]\n",
    "epochs = [;19,22]\n",
    "batch_size = [128,256]\n",
    "param_grid = dict(neurons=neurons, neurons2=neurons2,neurons3=neurons3, epochs=epochs,batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\n",
    "grid_result = grid.fit\"\"(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after you discover / tune the hyperparameters of your deep learning networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create  the model and train and validate on valiation data save the best weights \n",
    "by watching the validation logarthmic loss ,monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =create_model(1000,100,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check=ModelCheckpoint(filepath='best.hd5',save_best_only=True,mode='min',monitor='val_loss',verbose=1)\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducelr=ReduceLROnPlateau(mode='min',verbose=1,patience=1,factor=0.97) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41458 samples, validate on 20420 samples\n",
      "Epoch 1/150\n",
      "41216/41458 [============================>.] - ETA: 0s - loss: 1.4793 - acc: 0.6328Epoch 00000: val_loss improved from inf to 1.48786, saving model to best.hd5\n",
      "41458/41458 [==============================] - 9s - loss: 1.4767 - acc: 0.6332 - val_loss: 1.4879 - val_acc: 0.6825\n",
      "Epoch 2/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.9235 - acc: 0.7132Epoch 00001: val_loss improved from 1.48786 to 0.89081, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.9206 - acc: 0.7138 - val_loss: 0.8908 - val_acc: 0.7310\n",
      "Epoch 3/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.7942 - acc: 0.7295Epoch 00002: val_loss improved from 0.89081 to 0.70583, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.7943 - acc: 0.7294 - val_loss: 0.7058 - val_acc: 0.7473\n",
      "Epoch 4/150\n",
      "39936/41458 [===========================>..] - ETA: 0s - loss: 0.7397 - acc: 0.7370Epoch 00003: val_loss improved from 0.70583 to 0.64697, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.7392 - acc: 0.7369 - val_loss: 0.6470 - val_acc: 0.7563\n",
      "Epoch 5/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.7077 - acc: 0.7452Epoch 00004: val_loss improved from 0.64697 to 0.61872, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.7073 - acc: 0.7453 - val_loss: 0.6187 - val_acc: 0.7684\n",
      "Epoch 6/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.7517Epoch 00005: val_loss improved from 0.61872 to 0.60242, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6803 - acc: 0.7522 - val_loss: 0.6024 - val_acc: 0.7722\n",
      "Epoch 7/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6614 - acc: 0.7558Epoch 00006: val_loss improved from 0.60242 to 0.58234, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6607 - acc: 0.7562 - val_loss: 0.5823 - val_acc: 0.7788\n",
      "Epoch 8/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.7596Epoch 00007: val_loss improved from 0.58234 to 0.57042, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6446 - acc: 0.7598 - val_loss: 0.5704 - val_acc: 0.7829\n",
      "Epoch 9/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.7657Epoch 00008: val_loss improved from 0.57042 to 0.56323, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6283 - acc: 0.7665 - val_loss: 0.5632 - val_acc: 0.7908\n",
      "Epoch 10/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.7706Epoch 00009: val_loss improved from 0.56323 to 0.55752, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6199 - acc: 0.7708 - val_loss: 0.5575 - val_acc: 0.7899\n",
      "Epoch 11/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.7737Epoch 00010: val_loss improved from 0.55752 to 0.55071, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6051 - acc: 0.7739 - val_loss: 0.5507 - val_acc: 0.7921\n",
      "Epoch 12/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.7750Epoch 00011: val_loss improved from 0.55071 to 0.54339, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.6030 - acc: 0.7750 - val_loss: 0.5434 - val_acc: 0.7945\n",
      "Epoch 13/150\n",
      "39936/41458 [===========================>..] - ETA: 0s - loss: 0.5936 - acc: 0.7772Epoch 00012: val_loss improved from 0.54339 to 0.54047, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.5936 - acc: 0.7769 - val_loss: 0.5405 - val_acc: 0.7950\n",
      "Epoch 14/150\n",
      "40192/41458 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.7822Epoch 00013: val_loss improved from 0.54047 to 0.53642, saving model to best.hd5\n",
      "41458/41458 [==============================] - 1s - loss: 0.5864 - acc: 0.7821 - val_loss: 0.5364 - val_acc: 0.7974\n",
      "Epoch 15/150\n",
      "39936/41458 [===========================>..] - ETA: 0s - loss: 0.5764 - acc: 0.7820"
     ]
    }
   ],
   "source": [
    "callbacks_list = [check,reducelr]\n",
    "model.fit(X, y, validation_split=0.33, epochs=150, batch_size=256,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
