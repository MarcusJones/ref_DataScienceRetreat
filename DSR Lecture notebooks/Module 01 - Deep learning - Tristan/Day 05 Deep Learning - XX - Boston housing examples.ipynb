{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to get feature names?????\n",
    "from sklearn.datasets import load_boston\n",
    "other_dataset = load_boston()\n",
    "df = pd.DataFrame(other_dataset.data, columns=other_dataset.feature_names)\n",
    "df['target'] = other_dataset.target\n",
    "feature_names = other_dataset.feature_names\n",
    "print(other_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = ks.datasets.boston_housing.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracting the mean: [  3.74511057e+00   1.14801980e+01   1.11044307e+01   6.18811881e-02\n",
      "   5.57355941e-01   6.26708168e+00   6.90106436e+01   3.74027079e+00\n",
      "   9.44059406e+00   4.05898515e+02   1.84759901e+01   3.54783168e+02\n",
      "   1.27408168e+01]\n",
      "Dividing by the std: [  9.22929073e+00   2.37382770e+01   6.80287253e+00   2.40939633e-01\n",
      "   1.17147847e-01   7.08908627e-01   2.79060634e+01   2.02770050e+00\n",
      "   8.68758849e+00   1.66168506e+02   2.19765689e+00   9.39946015e+01\n",
      "   7.24556085e+00]\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = ks.datasets.boston_housing.load_data()\n",
    "\n",
    "## Normalize the data, by column\n",
    "mean= train_data.mean(axis=0)\n",
    "print(\"Subtracting the mean:\", mean)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "print(\"Dividing by the std:\", std)\n",
    "train_data /= std\n",
    "\n",
    "# Apply normalization to the test set as well\n",
    "test_data -= mean\n",
    "test_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  404  examples, with 13 features\n",
      "Training output:  (404,)\n",
      "\n",
      "Test data:  102  examples, with 13 features\n",
      "Test output:  (404,)\n",
      "Test data shape:  (102, 13)\n",
      "Train data 0: [-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Maximum length: 13\n",
      "Average length: 13.0\n",
      "Largest index: 2.991784193632328\n"
     ]
    }
   ],
   "source": [
    "lengths = list()\n",
    "max_indices = list()\n",
    "for el in train_data[0:5]:\n",
    "    lengths.append(len(el))\n",
    "    max_indices.append(max(el))\n",
    "print(\"Training data: \",train_data.shape[0],\" examples, with\",train_data.shape[1], \"features\")\n",
    "print(\"Training output: \",train_targets.shape)\n",
    "print(\"\")\n",
    "print(\"Test data: \",test_data.shape[0],\" examples, with\",test_data.shape[1], \"features\")\n",
    "print(\"Test output: \",train_targets.shape)\n",
    "\n",
    "print(\"Test data shape: \",test_data.shape)\n",
    "print(\"Train data 0: {}\".format(train_data[0]))\n",
    "print(\"Type: {}\".format(type(train_data[0])))\n",
    "print(\"Maximum length: {}\".format(max(lengths)))\n",
    "print(\"Average length: {}\".format(np.mean(lengths)))\n",
    "print(\"Largest index: {}\".format(max(max_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_df = pd.DataFrame(train_data, )\n",
    "this_df.columns = feature_names\n",
    "#this_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAIR PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(this_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.models\n",
    "ks.layers\n",
    "\n",
    "def build_model():\n",
    "    model = ks.models.Sequential()\n",
    "    model.add(ks.layers.Dense(64, activation='relu', input_shape = (train_data.shape[1],)))\n",
    "    model.add(ks.layers.Dense(64, activation='relu'))\n",
    "    model.add(ks.layers.Dense(1))\n",
    "    \n",
    "    optimizer = 'rmsprop'\n",
    "    loss = 'mse' # Mean squared error\n",
    "    metrics = ['mae']\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics = metrics)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 4\n",
      "num_val_samples 101\n",
      "Fold: 0\n",
      "(101, 13) (101,)\n",
      "(303, 13) (303,)\n",
      "Fold: 1\n",
      "(101, 13) (101,)\n",
      "(303, 13) (303,)\n",
      "Fold: 2\n",
      "(101, 13) (101,)\n",
      "(303, 13) (303,)\n",
      "Fold: 3\n",
      "(101, 13) (101,)\n",
      "(303, 13) (303,)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "print(\"k:\", k)\n",
    "num_val_samples = len(train_data) // k\n",
    "print(\"num_val_samples\",num_val_samples)\n",
    "num_epochs = 100\n",
    "all_scores = list()\n",
    "all_history = list()\n",
    "for i in range(k):\n",
    "    print(\"Fold:\",i)\n",
    "    val_data = train_data[i*num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i*num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    left_data = train_data[:i * num_val_samples]\n",
    "    right_data = train_data[(i+1)  * num_val_samples:]\n",
    "    partial_train_data = np.concatenate([left_data, right_data], axis=0)\n",
    "    \n",
    "    left_targets = train_targets[:i * num_val_samples]\n",
    "    right_targets = train_targets[(i+1)  * num_val_samples:]\n",
    "    partial_train_targets = np.concatenate([left_targets, right_targets], axis=0)\n",
    "    \n",
    "    print(val_data.shape, val_targets.shape)\n",
    "    print(partial_train_data.shape, partial_train_targets.shape)\n",
    "    \n",
    "    model = build_model()\n",
    "    this_history = model.fit(\n",
    "        partial_train_data, \n",
    "        partial_train_targets,\n",
    "        epochs = num_epochs,\n",
    "        batch_size = 1,\n",
    "        verbose = False)\n",
    "    \n",
    "    #print(val_data)\n",
    "    #print(val_targets)\n",
    "    \n",
    "    #print(type(val_data))\n",
    "    #print(type(val_targets))\n",
    "    \n",
    "    #val_mse, val_mae = \n",
    "    result = model.evaluate(val_data, val_targets, verbose = 0)\n",
    "    all_history.append(this_history)\n",
    "    all_scores.append(result)\n",
    "print(\"Done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.4497931546503953, 2.1677483497279706], [12.430517952040871, 2.5807215081583155], [14.788055384513175, 2.5327313949566075], [12.908984082760197, 2.5581954906482509]]\n",
      "Mean Squared Error: 12.1443376435 Absolute Error: 2.45984918587\n"
     ]
    }
   ],
   "source": [
    "# All scores\n",
    "print(all_scores)\n",
    "mean_scores = np.mean(all_scores,axis = 0)\n",
    "print(\"Mean Squared Error:\",mean_scores[0],\"Absolute Error:\",mean_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
