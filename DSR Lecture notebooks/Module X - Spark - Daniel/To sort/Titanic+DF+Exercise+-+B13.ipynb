{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-4bc56731f3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Load the train and test sets\n",
    "- Check the schema, the variables have their right types?\n",
    "- If not, how to correctly load the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "customSchema = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Survived\", DoubleType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "customSchema2 = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True), \n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "train = sqlc.read.csv(\"train.csv\", header=True, schema=customSchema)\n",
    "test = sqlc.read.csv(\"test.csv\", header=True, schema=customSchema2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features of your dataset\n",
    "- You can use DataFrame's ***describe*** method to get summary statistics\n",
    "    - hint: ***toPandas*** may be useful to ease the manipulation of small dataframes\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: ***na*** property of DataFrames provide several methods of handling NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.3838383838383838</td>\n",
       "      <td>2.308641975308642</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.69911764705882</td>\n",
       "      <td>0.5230078563411896</td>\n",
       "      <td>0.38159371492704824</td>\n",
       "      <td>260318.54916792738</td>\n",
       "      <td>32.2042079685746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev</th>\n",
       "      <td>257.3538420152301</td>\n",
       "      <td>0.48659245426485753</td>\n",
       "      <td>0.8360712409770491</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14.526497332334035</td>\n",
       "      <td>1.1027434322934315</td>\n",
       "      <td>0.8060572211299488</td>\n",
       "      <td>471609.26868834975</td>\n",
       "      <td>49.69342859718089</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WE/P 5735</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PassengerId             Survived              Pclass  \\\n",
       "summary                                                               \n",
       "count                  891                  891                 891   \n",
       "mean                 446.0   0.3838383838383838   2.308641975308642   \n",
       "stddev   257.3538420152301  0.48659245426485753  0.8360712409770491   \n",
       "min                      1                  0.0                   1   \n",
       "max                    891                  1.0                   3   \n",
       "\n",
       "                                                     Name     Sex  \\\n",
       "summary                                                             \n",
       "count                                                 891     891   \n",
       "mean                                                 None    None   \n",
       "stddev                                               None    None   \n",
       "min      \"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"  female   \n",
       "max                           van Melkebeke, Mr. Philemon    male   \n",
       "\n",
       "                        Age               SibSp                Parch  \\\n",
       "summary                                                                \n",
       "count                   714                 891                  891   \n",
       "mean      29.69911764705882  0.5230078563411896  0.38159371492704824   \n",
       "stddev   14.526497332334035  1.1027434322934315   0.8060572211299488   \n",
       "min                    0.42                   0                    0   \n",
       "max                    80.0                   8                    6   \n",
       "\n",
       "                     Ticket               Fare Cabin Embarked  \n",
       "summary                                                        \n",
       "count                   891                891   204      889  \n",
       "mean     260318.54916792738   32.2042079685746  None     None  \n",
       "stddev   471609.26868834975  49.69342859718089  None     None  \n",
       "min                  110152                0.0   A10        C  \n",
       "max               WE/P 5735           512.3292     T        S  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating summary statistics and turning it into Pandas DF\n",
    "train_desc = train.describe().toPandas().set_index('summary')\n",
    "train_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('Embarked').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pclass': -0.3384810359610151, 'Age': 0.010539215871285682, 'Fare': 0.2573065223849626, 'Parch': 0.08162940708348339, 'SibSp': -0.0353224988857356}\n"
     ]
    }
   ],
   "source": [
    "# Computing correlations between Survived and some features\n",
    "print({col:train.stat.corr('Survived',col) \n",
    "       for col in ['Pclass','Age','SibSp','Parch','Fare']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pclass': 0, 'Ticket': 0, 'Fare': 0, 'SibSp': 0, 'Name': 0, 'Parch': 0, 'PassengerId': 0, 'Age': 177, 'Embarked': 2, 'Cabin': 687, 'Sex': 0, 'Survived': 0}\n"
     ]
    }
   ],
   "source": [
    "# Checking which columns have NULL values\n",
    "print({col:train.where(train[col].isNull()).count() \n",
    "       for col in train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69911764705882\n"
     ]
    }
   ],
   "source": [
    "# Taking the mean age from the Pandas DF\n",
    "ageMean = float(train_desc.loc['mean']['Age'])\n",
    "print(ageMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling the Age in both train and test datasets\n",
    "trainFilled = train.na.fill({'Age': ageMean, 'Embarked': 'S'})\n",
    "testFilled = test.na.fill({'Age': ageMean, 'Embarked': 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|   Sex|PClass|          avg(age)|\n",
      "+------+------+------------------+\n",
      "|  male|     3|26.507588932806325|\n",
      "|female|     3|             21.75|\n",
      "|female|     1| 34.61176470588235|\n",
      "|female|     2|28.722972972972972|\n",
      "|  male|     2| 30.74070707070707|\n",
      "|  male|     1| 41.28138613861386|\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "train.groupby('Sex','PClass').agg(F.mean('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- How to handle categorical features?\n",
    "    - hint: check the Estimators and Transformers\n",
    "- Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "- Make sure to end up with a DataFrame with two columns: ***Survived*** and ***vFeatures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "indexer1 = (StringIndexer()\n",
    "           .setInputCol(\"Embarked\")\n",
    "           .setOutputCol(\"nEmbarked\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed1 = indexer1.fit(trainFilled).transform(trainFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|Embarked|nEmbarked|\n",
      "+--------+---------+\n",
      "|       S|      0.0|\n",
      "|       C|      1.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       Q|      2.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       C|      1.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       Q|      2.0|\n",
      "|       S|      0.0|\n",
      "|       S|      0.0|\n",
      "|       C|      1.0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed1.select('Embarked','nEmbarked').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer2 = (StringIndexer()\n",
    "           .setInputCol(\"Sex\")\n",
    "           .setOutputCol(\"nSex\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed2 = indexer2.fit(indexed1).transform(indexed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder1 = OneHotEncoder().setInputCol(\"nEmbarked\").setOutputCol(\"vEmbarked\")\n",
    "encoded1 = encoder1.transform(indexed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|nEmbarked|    vEmbarked|\n",
      "+---------+-------------+\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      1.0|(2,[1],[1.0])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      2.0|    (2,[],[])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      0.0|(2,[0],[1.0])|\n",
      "|      1.0|(2,[1],[1.0])|\n",
      "+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded1.select('nEmbarked','vEmbarked').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder2 = OneHotEncoder().setInputCol(\"nSex\").setOutputCol(\"vSex\").setDropLast(False)\n",
    "encoded2 = encoder2.transform(encoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------------+\n",
      "|   Sex|nSex|         vSex|\n",
      "+------+----+-------------+\n",
      "|  male| 0.0|(2,[0],[1.0])|\n",
      "|female| 1.0|(2,[1],[1.0])|\n",
      "|female| 1.0|(2,[1],[1.0])|\n",
      "|female| 1.0|(2,[1],[1.0])|\n",
      "|  male| 0.0|(2,[0],[1.0])|\n",
      "|  male| 0.0|(2,[0],[1.0])|\n",
      "|  male| 0.0|(2,[0],[1.0])|\n",
      "|  male| 0.0|(2,[0],[1.0])|\n",
      "|female| 1.0|(2,[1],[1.0])|\n",
      "|female| 1.0|(2,[1],[1.0])|\n",
      "+------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded2.select('Sex', 'nSex','vSex').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a VectorAssembler to put together all feature columns\n",
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    "                                       'Age',\n",
    "                                       'SibSp',\n",
    "                                       'Parch',\n",
    "                                       'Fare',\n",
    "                                       'vSex',\n",
    "                                       'vEmbarked'], \n",
    "                            outputCol='vFeatures')\n",
    "\n",
    "assembled = assembler.transform(encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keeping only the features and label columns to \n",
    "assembled2 = assembled.select(\"Survived\",\"vFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|           vFeatures|\n",
      "+--------+--------------------+\n",
      "|     0.0|[3.0,22.0,1.0,0.0...|\n",
      "|     1.0|[1.0,38.0,1.0,0.0...|\n",
      "|     1.0|[3.0,26.0,0.0,0.0...|\n",
      "|     1.0|[1.0,35.0,1.0,0.0...|\n",
      "|     0.0|[3.0,35.0,0.0,0.0...|\n",
      "|     0.0|(9,[0,1,4,5],[3.0...|\n",
      "|     0.0|[1.0,54.0,0.0,0.0...|\n",
      "|     0.0|[3.0,2.0,3.0,1.0,...|\n",
      "|     1.0|[3.0,27.0,0.0,2.0...|\n",
      "|     1.0|[2.0,14.0,1.0,0.0...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- Apply a normalization Estimator of your choice to the ***features*** vector obtained in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().setInputCol(\"vFeatures\").setOutputCol(\"scaledFeat\").setWithStd(True).setWithMean(True)\n",
    "scalerModel = scaler.fit(assembled2)\n",
    "scaled = scalerModel.transform(assembled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|Survived|           vFeatures|          scaledFeat|\n",
      "+--------+--------------------+--------------------+\n",
      "|     0.0|[3.0,22.0,1.0,0.0...|[0.82691281652436...|\n",
      "|     1.0|[1.0,38.0,1.0,0.0...|[-1.5652278312782...|\n",
      "|     1.0|[3.0,26.0,0.0,0.0...|[0.82691281652436...|\n",
      "|     1.0|[1.0,35.0,1.0,0.0...|[-1.5652278312782...|\n",
      "|     0.0|[3.0,35.0,0.0,0.0...|[0.82691281652436...|\n",
      "|     0.0|(9,[0,1,4,5],[3.0...|[0.82691281652436...|\n",
      "|     0.0|[1.0,54.0,0.0,0.0...|[-1.5652278312782...|\n",
      "|     0.0|[3.0,2.0,3.0,1.0,...|[0.82691281652436...|\n",
      "|     1.0|[3.0,27.0,0.0,2.0...|[0.82691281652436...|\n",
      "|     1.0|[2.0,14.0,1.0,0.0...|[-0.3691575073769...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Instead of doing transformations on separate steps, put everything together with a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1, \n",
    "                            encoder2, \n",
    "                            assembler,\n",
    "                            scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainFilled)\n",
    "scaled = model.transform(trainFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|          scaledFeat|\n",
      "+--------+--------------------+\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     1.0|[-1.5652278312782...|\n",
      "|     1.0|[0.82691281652436...|\n",
      "|     1.0|[-1.5652278312782...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     0.0|[-1.5652278312782...|\n",
      "|     0.0|[0.82691281652436...|\n",
      "|     1.0|[0.82691281652436...|\n",
      "|     1.0|[-0.3691575073769...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled.select('Survived', 'scaledFeat').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the evaluators to find the Area Under ROC and Accuracy of your model\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Trains a RF classifier and make predictions\n",
    "rfC = RandomForestClassifier().setLabelCol(\"Survived\") \\\n",
    "                                .setFeaturesCol(\"scaledFeat\") \\\n",
    "                                .setNumTrees(50) \\\n",
    "        .setMaxDepth(10)\n",
    "\n",
    "model = rfC.fit(scaled)\n",
    "\n",
    "predictions = model.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------+\n",
      "|          scaledFeat|         probability|prediction|Survived|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|[0.82691281652436...|[0.88756433014797...|       0.0|     0.0|\n",
      "|[-1.5652278312782...|[0.00328787878787...|       1.0|     1.0|\n",
      "|[0.82691281652436...|[0.51233602722778...|       0.0|     1.0|\n",
      "|[-1.5652278312782...|[0.00379713804713...|       1.0|     1.0|\n",
      "|[0.82691281652436...|[0.88947540903175...|       0.0|     0.0|\n",
      "|[0.82691281652436...|[0.88520457183290...|       0.0|     0.0|\n",
      "|[-1.5652278312782...|[0.82292661054867...|       0.0|     0.0|\n",
      "|[0.82691281652436...|[0.86234908472870...|       0.0|     0.0|\n",
      "|[0.82691281652436...|[0.29421977887721...|       1.0|     1.0|\n",
      "|[-0.3691575073769...|[0.01261111111111...|       1.0|     1.0|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('scaledFeat', 'probability', 'prediction', 'Survived').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9, {0: 0.1276, 1: 0.1625, 2: 0.0524, 3: 0.0415, 4: 0.1664, 5: 0.203, 6: 0.2058, 7: 0.022, 8: 0.0188})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627073147349255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Defines an evaluator based on the metric areaUnderROC\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "# Evaluate the predictions\n",
    "roc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9034792368125701\n"
     ]
    }
   ],
   "source": [
    "ev2 = (MulticlassClassificationEvaluator()\n",
    "       .setLabelCol('Survived')\n",
    "       .setPredictionCol('prediction')\n",
    "       .setMetricName('accuracy'))\n",
    "\n",
    "acc = ev2.evaluate(predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "- Take a look at the test data - use DataFrame's ***createOrReplaceTempView*** method to perform SQL queries over the data\n",
    "    - hint: check if there are any NULL values in the dataset - if so, handle them\n",
    "- Apply the transformations to the test data\n",
    "    - hint: include the model to the pipeline\n",
    "- Make predictions using the model previously trained and the transformed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pclass': 0, 'Ticket': 0, 'Age': 0, 'Sex': 0, 'SibSp': 0, 'PassengerId': 0, 'Cabin': 327, 'Name': 0, 'Fare': 1, 'Parch': 0, 'Embarked': 0}\n"
     ]
    }
   ],
   "source": [
    "# Make the test set a \"table\"\n",
    "testFilled.createOrReplaceTempView('test')\n",
    "\n",
    "# Runs a series of SQL queries to get the number of null values in the test set\n",
    "print({col: sqlc.sql(\"select * from test where \" + col + \" is null\").count() for col in testFilled.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3701</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
       "0         1044       3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
       "\n",
       "   Fare Cabin Embarked  \n",
       "0  None  None        S  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, there is one null Fare, let's check it\n",
    "sqlc.sql(\"select * from test where Fare is null\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.675550101832997\n"
     ]
    }
   ],
   "source": [
    "# Since the Fare is highly dependent on the class, it makes more sense to use the average for the given class\n",
    "# But we need to take the average from the TRAINING set\n",
    "trainFilled.createOrReplaceTempView('train')\n",
    "avgFare = sqlc.sql(\"select mean(Fare) from train where Pclass = 3\").take(1)[0][0]\n",
    "print(avgFare)\n",
    "\n",
    "# Fill the missing value with the calculated average\n",
    "testFilled = testFilled.na.fill({'Fare': avgFare})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1, \n",
    "                            encoder2, \n",
    "                            assembler,\n",
    "                            scaler,\n",
    "                            rfC])\n",
    "\n",
    "model = pipeline.fit(trainFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testFilled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "\n",
    "- Load the answers for the ***test*** data\n",
    "- Combine it with your predictions into a single DataFrame\n",
    "- Use the evaluator you created on ***Step 6***\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = sqlc.read.csv('titanic_answers.csv', header=True)\n",
    "answers = answers.select('PassengerId',F.col('Survived').cast('Double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_answer = predictions.join(answers, on='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8179040895813053\n"
     ]
    }
   ],
   "source": [
    "roc = evaluator.evaluate(pred_answer)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7703349282296651\n"
     ]
    }
   ],
   "source": [
    "acc = ev2.evaluate(pred_answer)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
