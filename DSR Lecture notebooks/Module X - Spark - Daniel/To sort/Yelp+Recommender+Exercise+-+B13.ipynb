{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Recommender\n",
    "\n",
    "## Intro\n",
    "\n",
    "The purpose of this exercise is to use Spark in a real dataset, instead of just a toy example.\n",
    "\n",
    "You will use the data from the [Yelp Dataset Challenge](https://www.yelp.de/dataset_challenge), which contains information about businesses, users, reviews and more.\n",
    "\n",
    "For this exercise, you will need to focus only on the following files:\n",
    "- yelp_academic_dataset_business.json\n",
    "- yelp_academic_dataset_review.json\n",
    "\n",
    "The goal is to build a recommender using Spark's ALS (Alternating Least Squares) and then generate recommendations for a given user.\n",
    "\n",
    "Since the dataset is quite big, you should pick a business category (e.g. Restaurants) and a city (e.g. Edinburgh) and work on the recommender using only this subset of the data.\n",
    "\n",
    "Please take some time to:\n",
    "- find out what information you will need to feed as input to Spark's ALS\n",
    "- check how this information is available in the dataset\n",
    "- plan how you will tackle this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-620033e9ad33>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b7a3e070fdec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(pyspark.__version__)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msqlc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-620033e9ad33>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "#print(pyspark.__version__)\n",
    "sc = SparkContext('local[*]')\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_business.json*** and select the following columns:\n",
    "    - business_id\n",
    "    - name\n",
    "    - city\n",
    "    - stars\n",
    "    - categories\n",
    "    - address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/batman/git/ref_DataScienceRetreat/DSR Lecture notebooks/Module X - Spark - Daniel/To sort/yelp_academic_dataset_business.json;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/batman/git/ref_DataScienceRetreat/DSR Lecture notebooks/Module X - Spark - Daniel/To sort/yelp_academic_dataset_business.json;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:360)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:348)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:348)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-def96f6eb890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_business\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_academic_dataset_business.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m df_business = df_business.select('business_id',\n\u001b[1;32m      3\u001b[0m                                  \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0;34m'city'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0;34m'stars'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/batman/git/ref_DataScienceRetreat/DSR Lecture notebooks/Module X - Spark - Daniel/To sort/yelp_academic_dataset_business.json;'"
     ]
    }
   ],
   "source": [
    "df_business = sqlc.read.json('yelp_academic_dataset_business.json')\n",
    "df_business = df_business.select('business_id',\n",
    "                                 'name',\n",
    "                                 'city',\n",
    "                                 'stars',\n",
    "                                 'categories',\n",
    "                                 'address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='0DI8Dt2PJp07XkVvIElIcQ', name='Innovative Vapors', city='Tempe', stars=4.5, categories=['Tobacco Shops', 'Nightlife', 'Vape Shops', 'Shopping'], address='227 E Baseline Rd, Ste J2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a business category\n",
    "\n",
    "- Define a regular Python function that takes a list of categories and returns 1 if a category of your choice (for instance, 'Restaurants') is contained in the list of categories or 0 otherwise\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with an IntegerType return\n",
    "- Using the UDF, filter the businesses that belong to the category you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def is_category_listed(name, categories):\n",
    "    listed = 0\n",
    "    if categories is not None:\n",
    "        if name in categories:\n",
    "            listed = 1\n",
    "    return listed\n",
    "\n",
    "def is_restaurant(categories):\n",
    "    return is_category_listed('Restaurants', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_is_restaurant = UserDefinedFunction(is_restaurant, \n",
    "                                        IntegerType())\n",
    "\n",
    "df_restaurants = df_business.withColumn('is_restaurant', udf_is_restaurant('categories')) \\\n",
    "                            .filter('is_restaurant = 1') \\\n",
    "                            .drop('is_restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='EDqCEAGXVGCH4FJXgqtjqg', name='Pizza Pizza', city='Toronto', stars=2.5, categories=['Restaurants', 'Pizza', 'Chicken Wings', 'Italian'], address='979 Bloor Street W')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_restaurants.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UDF approach works just fine, but there is a more straightforward way to perform the same operation\n",
    "    - hint: look at ***array_contains*** SQL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# you can overwrite the former df_restaurants\n",
    "df_restaurants = (df_business\n",
    "                  .filter(F.array_contains('categories',\n",
    "                                           'Restaurants')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a city\n",
    "- Having filtered by the business category, now it is time to filter by the city (for instance, Edinburgh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_city_restaurants = df_restaurants.filter('city = \"Edinburgh\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "- If you haven't done it yet, take one sample from your already filtered DataFrame and notice that the ***business_id*** contains an alphanumeric value - this is not good for Spark's ALS implementation, which requires IDs for items (in our case, businesses) and users to be numeric\n",
    "- Use a ***StringIndexer*** to create a new column ***business_idn*** from the conversion of business_id into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "business_indexer = StringIndexer().setInputCol('business_id') \\\n",
    "                                  .setOutputCol('business_idn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_idx_model = business_indexer.fit(df_city_restaurants)\n",
    "df_city_restaurants = business_idx_model.transform(df_city_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='NsarUMMMPOlMBb6K04x6hw', name='Juice Almighty', city='Edinburgh', stars=4.5, categories=['Food', 'Fast Food', 'Restaurants', 'Juice Bars & Smoothies'], address='7A Castle Street, Corstorphine', business_idn=24.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, name: string, city: string, stars: double, categories: array<string>, address: string, business_idn: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_review.json*** and select the following columns:\n",
    "    - user_id\n",
    "    - business-id\n",
    "    - stars\n",
    "    - date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reviews = sqlc.read.json('yelp_academic_dataset_review.json')\n",
    "df_reviews = df_reviews.select('user_id',\n",
    "                               'business_id',\n",
    "                               'stars',\n",
    "                               'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping reviews for the chosen city only\n",
    "\n",
    "- You are only interested in reviews of businesses you kept after filtering for category and city - how to filter out everything else? (hint: take a look at the ***join*** operation of DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_city_reviews = (df_reviews\n",
    "                   .join(df_city_restaurants\n",
    "                         .select('business_id', 'business_idn'),\n",
    "                         on='business_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "\n",
    "- As it happened with the ***business_id***, you also need to convert ***user_id*** into a numeric value - once again, use a ***StringIndexer*** to create a new column named ***user_idn*** containing the result of the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_indexer = StringIndexer().setInputCol('user_id') \\\n",
    "                              .setOutputCol('user_idn').setHandleInvalid('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_idx_model = user_indexer.fit(df_city_reviews)\n",
    "df_city_reviews = user_idx_model.transform(df_city_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, stars: bigint, date: string, business_idn: double, user_idn: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_reviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a sequential number to the user's reviews\n",
    "\n",
    "- Now add a ***sequential number*** to the user's reviews, that is, for each user, order his/her reviews by date (multiple reviews on the same date can be randomly ordered) and number them (hint: check ***window functions***)\n",
    "- This sequential number will be useful later to perform a time-wise split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window().partitionBy('user_idn').orderBy('date')\n",
    "df_city_reviews = (df_city_reviews\n",
    "                   .withColumn('review_seq', F.row_number().over(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting reviews to keep only users with more than 4 reviews\n",
    "\n",
    "- Some users had rated only 1 or a few businesses - this would pose as a problem to make recommendations - so you would want to keep only users who had rated more than 4 reviews, for instance\n",
    "- Find the ***total number of reviews*** for each user and then filter them using this information (hint: again, you can use a ***window function***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = Window().partitionBy('user_idn')\n",
    "df_city_reviews = (df_city_reviews\n",
    "                   .withColumn('n_reviews', \n",
    "                               F.max('review_seq').over(w)))\n",
    "df_selected = df_city_reviews.filter('n_reviews > 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, stars: bigint, date: string, business_idn: double, user_idn: double, review_seq: int, n_reviews: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating mean rating by user\n",
    "\n",
    "- Now you can calculate the mean rating by user and make it into a dictionary where the key is the ***user_id*** (hint: look at ***rdd*** method of DataFrames and ***collectAsMap*** method of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_user_means = df_selected.select('user_id', 'stars') \\\n",
    "                             .groupby('user_id') \\\n",
    "                             .mean() \\\n",
    "                             .rdd \\\n",
    "                             .collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering rating by user\n",
    "\n",
    "- The dictionary containing mean ratings by user can be seen as a ***lookup table*** - what is the appropriate way of dealing with those in Spark?\n",
    "- Once you have figured this out, define a regular Python function that takes two arguments - ***user_id*** (String) and ***rating*** (String, which you will need to convert to float inside the function) - and returns the result of subtracting the mean rating of the user from the rating parameter\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with a DoubleType return\n",
    "- Using the UDF, create a column in your DataFrame with the centered ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "lookup_user_means = sc.broadcast(dict_user_means)\n",
    "\n",
    "def zero_mean(user_id, rating):\n",
    "    return (float(rating) - lookup_user_means.value.get(user_id))\n",
    "\n",
    "udf_zero_mean = UserDefinedFunction(lambda cols: \n",
    "                                    zero_mean(cols[0], cols[1]) ,\n",
    "                                    DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "df_centered = df_selected.withColumn('centered',\n",
    "                                     udf_zero_mean(array('user_id','stars'))) \\\n",
    "                .drop('stars') \\\n",
    "                .withColumnRenamed('centered', 'stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once again, the UDF approach is not the most \"Sparkonic\" way of handling this - can you perform the same operation using only functions from ***pyspark.sql.functions*** (which was imported earlier as F)?\n",
    "    - hint: you'll need ***Window functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can overwrite df_centered\n",
    "w = Window.partitionBy('user_id')\n",
    "df_centered = (df_selected\n",
    "               .withColumn('avg_stars', F.avg('stars').over(w))\n",
    "               .withColumn('stars', F.expr('stars - avg_stars'))\n",
    "               .drop('avg_stars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Even better, doing a groupby will do the trick\n",
    "df_centered = (df_selected\n",
    " .join(df_selected\n",
    "       .groupBy('user_id')\n",
    "       .agg({'stars': 'avg'}),\n",
    "       on='user_id')\n",
    " .withColumn('stars', F.col('stars') - F.col('avg(stars)'))\n",
    " .drop('avg(stars)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and test sets by time\n",
    "\n",
    "- In recommender systems, it is common practice to do the training/test split timewise, that is, the test set is composed of the latest reviews\n",
    "- First, filter only those reviews which have a sequential number smaller than the ***total number of reviews***, by user: this is your training set\n",
    "- Then, filter only those reviews which have a sequential number identical to the ***total number of reviews***, by user: this is your test set\n",
    "- Now you can see why you had to add a sequential number to the user's reiews - since some users had done all his/her reviews on the same day, you need to disambiguate them to split the dataset. By doing this, you guarantee your test set will have only 1 review for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df_centered.filter('review_seq < n_reviews')\n",
    "df_test = df_centered.filter('review_seq = n_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using Spark 2.1 (as in the Docker image), you need to filter out \"new\" businesses in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "businesses = df_training.select('business_id').distinct()\n",
    "df_test = df_test.join(businesses, on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Least Squares (ALS) Model\n",
    "\n",
    "- This is the recommender itself - the ALS uses a iterative approach to find the underlying factors that yield the user/item rating matrix\n",
    "- It takes as input a DataFrame with three columns, representing:\n",
    "    - userCol: user IDs (numeric - remember the conversion you did)\n",
    "    - itemCol: item IDs (numeric - remember the conversion you did)\n",
    "    - ratingCol: rating (numeric, obviously)\n",
    "    - coldStartStrategy: \"drop\" (if there is unseen data on the test set, meaning a new user/business, drop it) - ***only available from Spark 2.2 on***\n",
    "- Its parameters are:\n",
    "    - rank: the number of factors to consider\n",
    "    - maxIter: the maximum number of iterations to perform\n",
    "    - regParam: the regularization parameter\n",
    "- Use Spark's ALS to fit a model based on your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "params = {'rank': 5, \n",
    "          'maxIter': 10, \n",
    "          'regParam': 0.3}\n",
    "\n",
    "als = ALS(userCol=\"user_idn\",\n",
    "          itemCol=\"business_idn\", \n",
    "          ratingCol=\"stars\", \n",
    "          #coldStartStrategy=\"drop\",\n",
    "          seed=42).setParams(**params)\n",
    "\n",
    "model = als.fit(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the training set\n",
    "\n",
    "- Once the model is trained, make predictions for the training set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7314300918770551\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_training)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=\"stars\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(train_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the test set\n",
    "\n",
    "- Now, make predictions for the test set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9255248702342439\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_test)\n",
    "\n",
    "test_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+\n",
      "|         business_id|             user_id|               stars|  prediction|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "|K8D_ltZWGvdXUaSE7...|FN4UY8IjXRZYHsO9Y...|                -1.0|-0.016736459|\n",
      "|EMzpWGROM3-IPvcwf...|sJ-LZrznwzO49Syt_...| -0.3636363636363633|-0.090746984|\n",
      "|t_COIFO2ZdAxQyhOb...|bcxcQhp0sKYd9eUnE...|  1.0945945945945947|  0.16284901|\n",
      "|nQ_bo9d_CeO4gf19v...|5l1oONEcJYr5T3M7C...|              -2.125|  -0.2589333|\n",
      "|iyBlON0Ipc0kbF-YO...|V5U4-iDA2FeO2AlkU...|  -1.710526315789474|  -0.1968693|\n",
      "|pdrWG2TVIXyRR_pN9...|sKpXijbd_UpoDzVi3...| -0.3999999999999999|-8.241236E-4|\n",
      "|8axxI33rUyswfRk_v...|Ch-PCp_dby_Twyrdd...|                 0.0| -0.15129477|\n",
      "|8axxI33rUyswfRk_v...|GR5J2C2nhDZQuEsqy...|  0.3636363636363633| -0.07208337|\n",
      "|8axxI33rUyswfRk_v...|3lr8Yntb-NtzODxiX...|                 0.0|  0.17770714|\n",
      "|8axxI33rUyswfRk_v...|x_B6HetResuFzznXn...|                0.75| 0.039731197|\n",
      "|KL-HPc-8AvUZgMNvR...|ZwLfrWgTq7uFBQMEj...|  0.7999999999999998| -0.12409547|\n",
      "|kUQhcKnyUjd9Gcvxw...|F4CcgdflZCkR1_DBe...|                 0.5|  0.44000745|\n",
      "|g_fGPZJlGeKGLgC-j...|-AMiTsraRXFdNX9yB...| -1.2222222222222223|  0.14306293|\n",
      "|89VzbB2jkOle19uIg...|zUXTdPXno9-V_LVXO...|               1.375| -0.64674735|\n",
      "|89VzbB2jkOle19uIg...|oyWRUMX7b2aNwzdvg...| 0.20000000000000018| 0.037491594|\n",
      "|LVKdUEcnFAPuLsfD_...|c7t_iq4kJl5unwp9T...|                 0.0|         0.0|\n",
      "|S9XKKA1evhRh16zrX...|VkOrDn_QYoq0x5E1O...| -0.8823529411764706|  0.04910129|\n",
      "|S9XKKA1evhRh16zrX...|yfXqZkU5iXE07GSHz...|  0.9900990099009901|  0.01155385|\n",
      "|S9XKKA1evhRh16zrX...|F7o19Xwgh-Qtg-8zg...|-0.22222222222222232| 0.049112294|\n",
      "|S9XKKA1evhRh16zrX...|G43n_TjBFQCGqfdCi...|-0.40000000000000036| 0.025586085|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('business_id','user_id','stars','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Now, your model is trained, but how can you use it to make recommendations for a given user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing business data\n",
    "\n",
    "- It would not make sense to recommend a place the user has already rated, right? So, generate a dictionary where ***user_idn*** is the key and a list of the already rated ***business_idn*** is the value (hint: when aggregating DataFrames, ***collect_list*** is a VERY useful function to turn multiple records into a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "dict_visited_by_user = df_city_reviews.select('user_idn', 'business_idn') \\\n",
    "                                      .groupby('user_idn') \\\n",
    "                                      .agg(collect_list('business_idn')) \\\n",
    "                                      .rdd \\\n",
    "                                      .collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Besides, recommending a given business_id also does not help much, right? So you need to organize the business data in a way it can be shown to the user.\n",
    "    - Define a regular Python function that takes one argument ***row*** (Row type) and returns a dictionary where ***business_idn*** is the key and the value is yet another dictionary with relevant fields (for instance: name, address, stars, categories)\n",
    "    - Transform your business DataFrame into an RDD and apply the function you defined - upon collecting, you will end up with a list of dictionaries\n",
    "    - Transform this list of dictionaries into a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rest_to_json(row):\n",
    "    return {row.business_idn: {'name': row.name, \n",
    "                               'address': row.address,\n",
    "                               'stars': row.stars, \n",
    "                               'categories': row.categories}}\n",
    "\n",
    "rest = df_city_restaurants.select('business_idn',\n",
    "                                  'name',\n",
    "                                  'address',\n",
    "                                  'stars',\n",
    "                                  'categories') \\\n",
    "                          .rdd \\\n",
    "                          .map(rest_to_json) \\\n",
    "                          .collect()\n",
    "\n",
    "dict_rest = {k: v for d in rest for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making recommendations for a user\n",
    "\n",
    "- To actually make the recommendations, we need to build an input DataFrame to feed the model\n",
    "    - A DataFrame can be created using the SQL Context and a list of Rows, each containg two columns: user_idn and business_idn - the rating will be computed by the model\n",
    "    - But you only need to have rows for the businesses which were not yet rated by the user - from all businesses, exclude the ones already rated by him/her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "user_idn = 317\n",
    "n_business = len(dict_rest)\n",
    "\n",
    "visited = dict_visited_by_user[user_idn]\n",
    "not_visited = list(set(range(n_business)).difference(set(visited)))\n",
    "\n",
    "rows_user = [Row(user_idn=user_idn, \n",
    "                 business_idn=float(i)) for i in not_visited]\n",
    "\n",
    "df_test_user = sqlc.createDataFrame(rows_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|business_idn|user_idn|\n",
      "+------------+--------+\n",
      "|         0.0|     317|\n",
      "|         1.0|     317|\n",
      "|         2.0|     317|\n",
      "|         3.0|     317|\n",
      "|         4.0|     317|\n",
      "|         5.0|     317|\n",
      "|         6.0|     317|\n",
      "|         7.0|     317|\n",
      "|         8.0|     317|\n",
      "|         9.0|     317|\n",
      "+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_user.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you can use the generated DataFrame to make predictions\n",
    "    - If there are any NA predictions, make sure to turn them into a really bad value (for instance, -5.0) (hint: remember ***na*** method of DataFrames)\n",
    "- Order the predictions and take the ***business_idn*** of the top 5\n",
    "- Finally, use this information to fetch the business data from the dictionary you assembled a couple of steps ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test_user).na.fill(-5.0)\n",
    "\n",
    "top_predictions = predictions.orderBy(desc('prediction')) \\\n",
    "                             .select('business_idn') \\\n",
    "                             .rdd \\\n",
    "                             .map(lambda row: row.business_idn) \\\n",
    "                             .take(5)\n",
    "\n",
    "response = list(map(lambda idn: dict_rest[idn], top_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'address': '8 Forrest Road',\n",
       "  'categories': ['Food',\n",
       "   'Sandwiches',\n",
       "   'Fast Food',\n",
       "   'Restaurants',\n",
       "   'Coffee & Tea'],\n",
       "  'name': \"Uncle T's\",\n",
       "  'stars': 5.0},\n",
       " {'address': '50-54 Henderson Street',\n",
       "  'categories': ['British', 'Restaurants'],\n",
       "  'name': 'Plumed Horse Restaurant',\n",
       "  'stars': 4.0},\n",
       " {'address': '11 Roseneath Street',\n",
       "  'categories': ['Scottish', 'French', 'Restaurants'],\n",
       "  'name': 'The Rabbit Hole',\n",
       "  'stars': 5.0},\n",
       " {'address': '54 Shore, Leith',\n",
       "  'categories': ['French', 'Restaurants', 'British'],\n",
       "  'name': 'Martin Wishart',\n",
       "  'stars': 5.0},\n",
       " {'address': '6 Gillespie Place, Bruntsfield',\n",
       "  'categories': ['Restaurants', 'Fast Food', 'African'],\n",
       "  'name': 'Khartoum Cafe',\n",
       "  'stars': 4.5}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you finished the exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_idn=471, recommendations=[Row(business_idn=575, rating=1.0452044010162354), Row(business_idn=423, rating=0.998054563999176)]),\n",
       " Row(user_idn=463, recommendations=[Row(business_idn=375, rating=0.5284951329231262), Row(business_idn=735, rating=0.5254964828491211)]),\n",
       " Row(user_idn=496, recommendations=[Row(business_idn=735, rating=0.21667301654815674), Row(business_idn=702, rating=0.20976904034614563)]),\n",
       " Row(user_idn=148, recommendations=[Row(business_idn=226, rating=1.0834600925445557), Row(business_idn=343, rating=0.8885278701782227)]),\n",
       " Row(user_idn=540, recommendations=[Row(business_idn=636, rating=0.6693418025970459), Row(business_idn=575, rating=0.6543372273445129)])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommendForAllUsers(2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (spark)\n",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
