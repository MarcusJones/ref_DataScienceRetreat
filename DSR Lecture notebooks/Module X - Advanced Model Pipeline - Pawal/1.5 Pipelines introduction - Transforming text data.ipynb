{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modules:\n",
      "Numpy                np              1.14.0\n",
      "Pandas               pd              0.22.0\n",
      "Keras                ks              2.0.6\n",
      "\n",
      "Matplotlib           mpl             2.1.2\n",
      "matplotlib.pyplot    plt             N/A\n",
      "matplotlib.image     mpimg           N/A\n",
      "Seaborn              sns             0.8.1\n",
      "PIL                  PIL             5.0.0\n",
      "\n",
      "ExergyUtilities      exergy          2.0.\n",
      "\n",
      "pyspark              pyspark         2.2.1\n"
     ]
    }
   ],
   "source": [
    "print_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Max Protection Solid Red Yugioh Sized Card Sle...</td>\n",
       "      <td>Collectibles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hippies Use Side Door metal sign         (ar)</td>\n",
       "      <td>Collectibles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REVERSE Embellished Hi Lo Bandeau Dress Black ...</td>\n",
       "      <td>Clothes, Shoes &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VAL72299   Game Color Set - Introduction Set  ...</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bang tidy decal sticker large 250mmx145mm dub ...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Max Protection Solid Red Yugioh Sized Card Sle...   \n",
       "1      Hippies Use Side Door metal sign         (ar)   \n",
       "2  REVERSE Embellished Hi Lo Bandeau Dress Black ...   \n",
       "3  VAL72299   Game Color Set - Introduction Set  ...   \n",
       "4  bang tidy decal sticker large 250mmx145mm dub ...   \n",
       "\n",
       "                  category_name  \n",
       "0                  Collectibles  \n",
       "1                  Collectibles  \n",
       "2  Clothes, Shoes & Accessories  \n",
       "3                  Toys & Games  \n",
       "4   Vehicle Parts & Accessories  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ebaytitles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "df = df.sample(frac=0.1) # delete this line if you are brave and have many GBs of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out unique values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jewellery & Watches', 'Home, Furniture & DIY',\n",
       "       'Vehicle Parts & Accessories', 'Sporting Goods', 'Toys & Games',\n",
       "       'Music', 'Crafts', 'Clothes, Shoes & Accessories', 'Collectibles',\n",
       "       'Sports Memorabilia', 'Garden & Patio',\n",
       "       'Computers/Tablets & Networking', 'Health & Beauty',\n",
       "       'Pet Supplies', 'Business, Office & Industrial',\n",
       "       'Mobile Phones & Communication', 'Dolls & Bears', 'Sound & Vision',\n",
       "       'Musical Instruments & Gear', 'DVDs, Films & TV',\n",
       "       'Consumer Electronics', 'Antiques', 'Art',\n",
       "       'Video Games & Consoles', 'Baby', 'Everything Else',\n",
       "       'Books, Comics & Magazines', 'Stamps', 'Cameras & Photography',\n",
       "       'Coins & Paper Money', 'Pottery, Porcelain & Glass',\n",
       "       'Cell Phones & Accessories', 'Wholesale & Job Lots',\n",
       "       'Holidays & Travel', 'Entertainment Memorabilia'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test observations - there is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663046            Jewellery & Watches\n",
       "115104          Home, Furniture & DIY\n",
       "354724    Vehicle Parts & Accessories\n",
       "148463          Home, Furniture & DIY\n",
       "106838          Home, Furniture & DIY\n",
       "418108            Jewellery & Watches\n",
       "587234          Home, Furniture & DIY\n",
       "222700                 Sporting Goods\n",
       "607791    Vehicle Parts & Accessories\n",
       "286580                   Toys & Games\n",
       "Name: category_name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.title.values\n",
    "y = df.category_name.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "------------------\n",
    "\n",
    "1. Count how many titles are in each category (```pandas.DataFrame.groupby```). Print out most common at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vehicle Parts &amp; Accessories</th>\n",
       "      <td>23177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clothes, Shoes &amp; Accessories</th>\n",
       "      <td>16747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home, Furniture &amp; DIY</th>\n",
       "      <td>12664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computers/Tablets &amp; Networking</th>\n",
       "      <td>6764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewellery &amp; Watches</th>\n",
       "      <td>6286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sporting Goods</th>\n",
       "      <td>4886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mobile Phones &amp; Communication</th>\n",
       "      <td>3963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crafts</th>\n",
       "      <td>3406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health &amp; Beauty</th>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toys &amp; Games</th>\n",
       "      <td>2979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business, Office &amp; Industrial</th>\n",
       "      <td>2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collectibles</th>\n",
       "      <td>2437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sound &amp; Vision</th>\n",
       "      <td>1862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garden &amp; Patio</th>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cameras &amp; Photography</th>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baby</th>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVDs, Films &amp; TV</th>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pet Supplies</th>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Art</th>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video Games &amp; Consoles</th>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books, Comics &amp; Magazines</th>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musical Instruments &amp; Gear</th>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Memorabilia</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dolls &amp; Bears</th>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coins &amp; Paper Money</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antiques</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Everything Else</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Electronics</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pottery, Porcelain &amp; Glass</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wholesale &amp; Job Lots</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stamps</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell Phones &amp; Accessories</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holidays &amp; Travel</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment Memorabilia</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title\n",
       "category_name                        \n",
       "Vehicle Parts & Accessories     23177\n",
       "Clothes, Shoes & Accessories    16747\n",
       "Home, Furniture & DIY           12664\n",
       "Computers/Tablets & Networking   6764\n",
       "Jewellery & Watches              6286\n",
       "Sporting Goods                   4886\n",
       "Mobile Phones & Communication    3963\n",
       "Crafts                           3406\n",
       "Health & Beauty                  3312\n",
       "Toys & Games                     2979\n",
       "Business, Office & Industrial    2841\n",
       "Collectibles                     2437\n",
       "Sound & Vision                   1862\n",
       "Music                            1349\n",
       "Garden & Patio                   1043\n",
       "Cameras & Photography             835\n",
       "Baby                              673\n",
       "DVDs, Films & TV                  599\n",
       "Pet Supplies                      592\n",
       "Art                               584\n",
       "Video Games & Consoles            494\n",
       "Books, Comics & Magazines         483\n",
       "Musical Instruments & Gear        439\n",
       "Sports Memorabilia                289\n",
       "Dolls & Bears                     280\n",
       "Coins & Paper Money               198\n",
       "Antiques                          185\n",
       "Everything Else                   155\n",
       "Consumer Electronics              133\n",
       "Pottery, Porcelain & Glass         94\n",
       "Wholesale & Job Lots               91\n",
       "Stamps                             83\n",
       "Cell Phones & Accessories          69\n",
       "Holidays & Travel                   4\n",
       "Entertainment Memorabilia           4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category_name').count().sort_values('title',ascending=False)\n",
    "#df.groupby('category_name').count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "print(frequencies)\n",
    "\n",
    "# or faster\n",
    "\n",
    "df.category_name.value_counts()\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vehicle Parts & Accessories       23177\n",
       "Clothes, Shoes & Accessories      16747\n",
       "Home, Furniture & DIY             12664\n",
       "Computers/Tablets & Networking     6764\n",
       "Jewellery & Watches                6286\n",
       "Sporting Goods                     4886\n",
       "Mobile Phones & Communication      3963\n",
       "Crafts                             3406\n",
       "Health & Beauty                    3312\n",
       "Toys & Games                       2979\n",
       "Business, Office & Industrial      2841\n",
       "Collectibles                       2437\n",
       "Sound & Vision                     1862\n",
       "Music                              1349\n",
       "Garden & Patio                     1043\n",
       "Cameras & Photography               835\n",
       "Baby                                673\n",
       "DVDs, Films & TV                    599\n",
       "Pet Supplies                        592\n",
       "Art                                 584\n",
       "Video Games & Consoles              494\n",
       "Books, Comics & Magazines           483\n",
       "Musical Instruments & Gear          439\n",
       "Sports Memorabilia                  289\n",
       "Dolls & Bears                       280\n",
       "Coins & Paper Money                 198\n",
       "Antiques                            185\n",
       "Everything Else                     155\n",
       "Consumer Electronics                133\n",
       "Pottery, Porcelain & Glass           94\n",
       "Wholesale & Job Lots                 91\n",
       "Stamps                               83\n",
       "Cell Phones & Accessories            69\n",
       "Entertainment Memorabilia             4\n",
       "Holidays & Travel                     4\n",
       "Name: category_name, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "#print(frequencies)\n",
    "\n",
    "# or faster\n",
    "\n",
    "df.category_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unisex Men's Women's UCC 50/50 Blend Adult Set-In Sweatshirt - UCC001 -IS A- Clothes, Shoes & Accessories\n",
      "KIA RIO 01-05 14\" LUXURY WHEEL TRIM HUB CAP SET SPARK BRAND NEW -IS A- Vehicle Parts & Accessories\n"
     ]
    }
   ],
   "source": [
    "cats = df.category_name.values\n",
    "print(X_tr[0],\"-IS A-\",y_tr[0])\n",
    "print(X_tr[-1],\"-IS A-\",y_tr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit it to a Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 72233)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_tr)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59143"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature index: \n",
    "count_vect.vocabulary_.get(u'shampoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FREQUENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 72233)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_vects = tfidf_vect.fit_transform(X_tr)\n",
    "X_train_vects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a mapping to the column indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.vocabulary_.get(u'shampoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 72233)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster, do both at once! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 72233)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pills shampoo CLASSIFIED AS Health & Beauty\n",
      "sport ball CLASSIFIED AS Vehicle Parts & Accessories\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['pills shampoo','sport ball']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(doc,\"CLASSIFIED AS\", category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigger', 'biggest', 'run', 'We', 'are', 'the', 'world', 'connect']\n"
     ]
    }
   ],
   "source": [
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('english')\n",
    "print(stemmer.stemWords(\"bigger biggest running We are the world connections\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla\n",
      "['00', '000', '0000', '00000', '000002', '000018', '000021', '00003f', '00003g', '000051446b']\n",
      "72233\n",
      "preprocessing\n",
      "['aa', 'aaa', 'aaaa', 'aaaaa', 'aaad', 'aabd', 'aac', 'aaci', 'aadipod', 'aai']\n",
      "39547\n",
      "preprocessing + min_df=10\n",
      "['aa', 'aaa', 'ab', 'abarth', 'abc', 'about', 'abr', 'abs', 'absolute', 'absorber']\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('english')\n",
    "\n",
    "def clean_text(t):\n",
    "    \"\"\"Accepts a Document \n",
    "    \"\"\"\n",
    "    t = t.lower()\n",
    "    # Remove single characters\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    # Replace all numbers by a single char\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    \n",
    "    return t\n",
    "\n",
    "def clean_text_stemmed(t):\n",
    "    \"\"\"Accepts a Document \n",
    "    \"\"\"\n",
    "    t = t.lower()\n",
    "    # Remove single characters\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    # Replace all numbers by a single char\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    tfinal = \" \".join(stemmer.stemWords(t.split()))\n",
    "    \n",
    "    return tfinal\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type | Clean text | Stemmed\n",
    "---------| ---------| ---------\n",
    "vanilla | 71866 | 71866\n",
    "preprocessing  | 39309 | 33307\n",
    "preprocessing + min_df=10 | 7499 | 6630"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "------------------\n",
    "\n",
    "Linguistic normalization in which variant forms are reduced to a common form\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "    \n",
    "Usage:\n",
    "\n",
    "    import snowballstemmer\n",
    "\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    print(stemmer.stemWords(\"We are the world\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False))])\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.25      0.05      0.08        22\n",
      "                           Art       0.63      0.59      0.61        58\n",
      "                          Baby       0.82      0.43      0.56        77\n",
      "     Books, Comics & Magazines       0.95      0.62      0.75        60\n",
      " Business, Office & Industrial       0.85      0.50      0.63       292\n",
      "         Cameras & Photography       0.86      0.82      0.84        74\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         5\n",
      "  Clothes, Shoes & Accessories       0.86      0.97      0.91      1682\n",
      "           Coins & Paper Money       1.00      0.72      0.84        18\n",
      "                  Collectibles       0.77      0.58      0.66       243\n",
      "Computers/Tablets & Networking       0.89      0.94      0.91       665\n",
      "          Consumer Electronics       0.50      0.07      0.12        14\n",
      "                        Crafts       0.83      0.81      0.82       336\n",
      "              DVDs, Films & TV       0.82      0.82      0.82        61\n",
      "                 Dolls & Bears       0.84      0.87      0.86        31\n",
      "               Everything Else       1.00      0.06      0.12        16\n",
      "                Garden & Patio       0.87      0.71      0.78       102\n",
      "               Health & Beauty       0.83      0.82      0.83       339\n",
      "         Home, Furniture & DIY       0.82      0.87      0.84      1286\n",
      "           Jewellery & Watches       0.90      0.97      0.93       660\n",
      " Mobile Phones & Communication       0.88      0.94      0.91       405\n",
      "                         Music       0.85      0.90      0.88       121\n",
      "    Musical Instruments & Gear       0.65      0.50      0.57        34\n",
      "                  Pet Supplies       1.00      0.63      0.78        60\n",
      "    Pottery, Porcelain & Glass       1.00      0.45      0.62        11\n",
      "                Sound & Vision       0.79      0.53      0.63       188\n",
      "                Sporting Goods       0.88      0.67      0.76       499\n",
      "            Sports Memorabilia       1.00      0.06      0.12        32\n",
      "                        Stamps       0.75      0.60      0.67         5\n",
      "                  Toys & Games       0.86      0.74      0.80       294\n",
      "   Vehicle Parts & Accessories       0.90      0.97      0.93      2255\n",
      "        Video Games & Consoles       0.89      0.83      0.86        47\n",
      "          Wholesale & Job Lots       0.60      0.38      0.46         8\n",
      "\n",
      "                   avg / total       0.86      0.86      0.85     10000\n",
      "\n",
      "0.8629\n",
      "1 <function clean_text at 0x7f973bdc3048>\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function clean_text at 0x7f...='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False))])\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.25      0.05      0.08        22\n",
      "                           Art       0.63      0.55      0.59        58\n",
      "                          Baby       0.83      0.44      0.58        77\n",
      "     Books, Comics & Magazines       0.93      0.63      0.75        60\n",
      " Business, Office & Industrial       0.82      0.48      0.61       292\n",
      "         Cameras & Photography       0.85      0.81      0.83        74\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         5\n",
      "  Clothes, Shoes & Accessories       0.86      0.97      0.91      1682\n",
      "           Coins & Paper Money       1.00      0.72      0.84        18\n",
      "                  Collectibles       0.77      0.59      0.67       243\n",
      "Computers/Tablets & Networking       0.90      0.94      0.92       665\n",
      "          Consumer Electronics       0.67      0.14      0.24        14\n",
      "                        Crafts       0.84      0.81      0.82       336\n",
      "              DVDs, Films & TV       0.83      0.82      0.83        61\n",
      "                 Dolls & Bears       0.84      0.87      0.86        31\n",
      "               Everything Else       0.00      0.00      0.00        16\n",
      "                Garden & Patio       0.86      0.73      0.79       102\n",
      "               Health & Beauty       0.82      0.82      0.82       339\n",
      "         Home, Furniture & DIY       0.81      0.86      0.84      1286\n",
      "           Jewellery & Watches       0.90      0.97      0.93       660\n",
      " Mobile Phones & Communication       0.88      0.94      0.91       405\n",
      "                         Music       0.85      0.91      0.88       121\n",
      "    Musical Instruments & Gear       0.72      0.53      0.61        34\n",
      "                  Pet Supplies       1.00      0.65      0.79        60\n",
      "    Pottery, Porcelain & Glass       0.71      0.45      0.56        11\n",
      "                Sound & Vision       0.76      0.51      0.61       188\n",
      "                Sporting Goods       0.87      0.68      0.76       499\n",
      "            Sports Memorabilia       1.00      0.12      0.22        32\n",
      "                        Stamps       0.75      0.60      0.67         5\n",
      "                  Toys & Games       0.86      0.73      0.79       294\n",
      "   Vehicle Parts & Accessories       0.90      0.98      0.94      2255\n",
      "        Video Games & Consoles       0.87      0.85      0.86        47\n",
      "          Wholesale & Job Lots       0.67      0.50      0.57         8\n",
      "\n",
      "                   avg / total       0.86      0.86      0.85     10000\n",
      "\n",
      "0.8623\n",
      "2 <function clean_text_stemmed at 0x7f973bdc3268>\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function clean_text_stemmed...='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False))])\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.40      0.09      0.15        22\n",
      "                           Art       0.60      0.57      0.58        58\n",
      "                          Baby       0.86      0.40      0.55        77\n",
      "     Books, Comics & Magazines       0.84      0.62      0.71        60\n",
      " Business, Office & Industrial       0.84      0.50      0.62       292\n",
      "         Cameras & Photography       0.85      0.81      0.83        74\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         5\n",
      "  Clothes, Shoes & Accessories       0.86      0.97      0.91      1682\n",
      "           Coins & Paper Money       0.93      0.78      0.85        18\n",
      "                  Collectibles       0.80      0.57      0.67       243\n",
      "Computers/Tablets & Networking       0.89      0.93      0.91       665\n",
      "          Consumer Electronics       0.67      0.14      0.24        14\n",
      "                        Crafts       0.83      0.78      0.81       336\n",
      "              DVDs, Films & TV       0.80      0.79      0.79        61\n",
      "                 Dolls & Bears       0.86      0.97      0.91        31\n",
      "               Everything Else       0.00      0.00      0.00        16\n",
      "                Garden & Patio       0.80      0.70      0.74       102\n",
      "               Health & Beauty       0.80      0.81      0.81       339\n",
      "         Home, Furniture & DIY       0.81      0.86      0.84      1286\n",
      "           Jewellery & Watches       0.90      0.97      0.94       660\n",
      " Mobile Phones & Communication       0.86      0.94      0.90       405\n",
      "                         Music       0.85      0.92      0.88       121\n",
      "    Musical Instruments & Gear       0.74      0.50      0.60        34\n",
      "                  Pet Supplies       0.97      0.63      0.77        60\n",
      "    Pottery, Porcelain & Glass       0.88      0.64      0.74        11\n",
      "                Sound & Vision       0.77      0.51      0.61       188\n",
      "                Sporting Goods       0.85      0.66      0.74       499\n",
      "            Sports Memorabilia       1.00      0.25      0.40        32\n",
      "                        Stamps       1.00      0.60      0.75         5\n",
      "                  Toys & Games       0.85      0.70      0.77       294\n",
      "   Vehicle Parts & Accessories       0.89      0.97      0.93      2255\n",
      "        Video Games & Consoles       0.88      0.81      0.84        47\n",
      "          Wholesale & Job Lots       0.67      0.25      0.36         8\n",
      "\n",
      "                   avg / total       0.85      0.86      0.85     10000\n",
      "\n",
      "0.858\n"
     ]
    }
   ],
   "source": [
    "preprocessors = [\n",
    "    None,\n",
    "    clean_text,\n",
    "    clean_text_stemmed,\n",
    "]\n",
    "for i,prep in enumerate(preprocessors):\n",
    "    print(i,prep)\n",
    "#     this_text_clf = Pipeline([('vect', CountVectorizer(preprocessor=prep)),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('clf', MultinomialNB()),])\n",
    "    this_text_clf = Pipeline([('vect', CountVectorizer(preprocessor=prep)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier()),])    \n",
    "    print(this_text_clf)                         \n",
    "    this_text_clf.fit(X_tr, y_tr)  \n",
    "    predicted = this_text_clf.predict(X_te)\n",
    "    acc=np.mean(predicted == y_te)\n",
    "    #print(classification_report(X_tr)\n",
    "    print(classification_report(y_te, predicted))\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_tr, y_tr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_te)\n",
    "np.mean(predicted == y_te)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', SGDClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds)\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 4.64158883e-04, 2.15443469e-02, 1.00000000e+00,\n",
       "       4.64158883e+01, 2.15443469e+03, 1.00000000e+05, 4.64158883e+06,\n",
       "       2.15443469e+08, 1.00000000e+10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-5,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-2)]: Done 135 out of 135 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-2,\n",
       "       param_grid={'vect__min_df': [1, 2, 5, 10, 15], 'clf__alpha': array([1.00000e-05, 7.49894e-04, 5.62341e-02, 4.21697e+00, 3.16228e+02,\n",
       "       2.37137e+04, 1.77828e+06, 1.33352e+08, 1.00000e+10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "params = {          'vect__min_df': [1,2,5,10,15],\n",
    "         'clf__alpha':np.logspace(-5,10,9)}\n",
    "    #'clf__alpha':np.logspace(-4,10,8)}\n",
    "grid_clf = GridSearchCV(text_clf, params, n_jobs=-2, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "#sklearn.grid_search.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623333333333333 {'clf__alpha': 0.0001, 'vect__min_df': 2}\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.best_score_,grid_clf.best_params_)\n",
    "grid_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-edfcfc093f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m                 for train, test in cv)\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tuned_parameters = {          'vect__min_df': [1,2,5,10],\n",
    "         'clf__alpha':np.logspace(-4,1,6)}\n",
    "\n",
    "tuned_parameters = {          'vect__min_df': [1],\n",
    "         'clf__alpha':np.logspace(-4,1,6)}\n",
    "\n",
    "#clf = grid_clf\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #clf = GridSearchCV(text_clf, tuned_parameters, cv=5, scoring=score)\n",
    "    #clf = GridSearchCV(text_clf, tuned_parameters, cv=5, scoring=score)\n",
    "    clf = GridSearchCV(text_clf, tuned_parameters, n_jobs=-2, scoring=score, verbose=True)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_estimator_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_te, y_pred = y_te, clf.predict(X_te)\n",
    "    print(classification_report(y_te, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, params, n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'model__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful materials\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "2. http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
